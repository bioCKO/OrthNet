#!/usr/bin/env python

import re,sys,os,subprocess
import argparse
from argparse import RawTextHelpFormatter


###################################################
### 0. script description and parsing arguments ###
###################################################
synopsis1 = "\
  - updates '.BestHitPairs' files, fixing OrthNet edges removed by mcl.\n\
  - runs 'CL_finder_multi.py' again with updated '.BestHitPairs' files.\n\
  - updates '.clstrd.edges(.nodes)' after mcl runs on OrthNets." # 80 characters per line 
synopsis2 = "detailed description:\n\
 0. Prerequisite:\n\
  - the script re-runs 'CL_finder_multi.py' using the updated 'BestHitPairs'\n\
     files. 'CL_finder_multi.py' and 'TDfiles' for all species are needed.\n\
 1. Input files and parameters:\n\
  - './<Project>.list' including all species IDs (spcsIDs), one per line, and\n\
     './<Project>.clstrd.edges(.nodes)', results of 'create_OrthNet.py', are\n\
     expected in the same directory.\n\
  - <mclOutput_parsed> is an output of 'parse_mclOutput.py'; i.e.\n\
     'nodeID' and 'clusterID', tab-delimited, with a header in the first row.\n\
  - '-i Path2Input': path to '<Project>.clstrd.edges(.nodes)' if not './'\n\
  - '-b Path2BHPs': path to 'BestHitPairs(BHPs)' files for all possible\n\
     species pairs with file names 'BestHits__spcsID1_vs_spcsID2.list'.\n\
  - '-t Path2TDfiles' defines the path to 'TDfiles', required for\n\
     re-running 'CL_finder_multi.py' (CLfm) with updated 'BestHitPairs' files.\n\
  - '-u'|'--updatedBHPs' assumes 'BestHitPairs' files generated by\n\
     'update_BestHitPairs.py' or 'updated_OrthNet_after_mcl.py' (itself).\n\
  - '-T TDfile_nameFmt': expects TDfiles named as spcsID + TDfile_nameFmt\n\
 2. Updating 'BestHitPairs' files:\n\
  - if the query and subject nodes in an edge are in different clusters after\n\
     mcl, search next best subjects for one in the same cluster with the query.\n\
  - update 'BestHitPairs' files the same way as 'update_BestHitPairs.py'. \n\
 3. Output files and options:\n\
  - '-o1 Path2BHPs_fxd': path to updated BHPs files (dafault='./Output'). \n\
  - '-o2 Path2Output': path to other output files (dafault='./Output'). \n\
  - Output file names for updated BHPs and '.clstrd.edges(.nodes) files include\n\
     '.afterMCL' in front of their extensions.\n\
  - OrthNet information is added as additional columns to the CLfm output files.\n\
  - print OrthNet nodes in 'mclOutput' format, which can be summarized and\n\
     annotated by 'parse_mclOutput.py'\n\
  - '-L'|'--Long': additional columns in '*.afterMCL.txt' output, for debugging\n\
     purpose; dafault=False.\n\
  - '-V'|'--Verbose': additional infos printed to .log, for debugging purpose\n\
     ; dafault=False.\n\
 4. Misc. details:\n\
  - 'BestHitPairs' can be tabulated blast+ results with the '-max_target_seqs N'\n\
     (N>=10) option; i.e. multiple subjects for each query, with the most \n\
     similar ('BestHit') followed by next best subjects.\n\
  - assumes no header in 'BestHitPairs' files.\n\n\
 by ohdongha@gmail.com ver0.3.9 20180515\n"
 
#version_history
#20180515 ver 0.3.9 # adds '-i' option to designate input folder 
#20180508 ver 0.3.8 # BHP file format changed slightly from 'BestHits__%s__vs__%s.list' to 'BestHits__%s_vs_%s.list' ... for some reason 
#20180324 ver 0.3.7 # print OrthNet nodes in ".mclOutput" format, which can be summarized and annotated by 'parse_mclOutput.py'  
#20180309 ver 0.3.6 # simplify default output and log files, print total # of nodes in the .nodeCounts file
#20180301 ver 0.3.5 # passing '-T' argument and '-r' option to CL_finder_multi.py
#20161026 ver 0.3.4 # adds "OrthNet_pattern" to the CLfm output files
#20161017 ver 0.3.3 # adds the total node number (#nodes) in the OrthNet to the CLfm output files; bug fixed
#20161003 ver 0.3.2 # adds OrthNet information to the CLfm output files.
#20160927 ver 0.3.1 # bug fixed 
#20160911 ver 0.3 # added the module to create updated .edges and .nodes files 
#20160825 ver 0.2 # deals with TD edges, reports updated # of BHpairs, and deals with iterated mcl runs, 
#20160812 ver 0.1 # fixing OrthNet edges removed by mcl
#20160623 ver 0.0
  
parser = argparse.ArgumentParser(description = synopsis1, epilog = synopsis2, formatter_class = RawTextHelpFormatter)

# positional arguments
parser.add_argument('Project', type=str, help="'./<Project>.list' includes spcsIDs being compared")
parser.add_argument('mclOutput_parsed', type=argparse.FileType('r'), help="results of mcl, parsed with 'parse_mclOutput.py'")
# options indicating PATHs
parser.add_argument('-i', dest="Path2Input", default='./', help="see below")
parser.add_argument('-t', dest="Path2TDfiles", type=str, default=".", help="PATH to 'TDfiles'; default='.'")
parser.add_argument('-T', dest="TDfile_nameFmt", type=str, default=".gtfParsed.pc.TD.txt", help="default='.gtfParsed.pc.TD.txt'; see below")
parser.add_argument('-b', dest="Path2BHPs", type=str, default=".", help="PATH to 'BestHitPairs' files; default='.'")
parser.add_argument('-o1', dest="Path2BHPs_fxd", type=str, default="./Output", help="PATH to updated BestHitPairs files") 
parser.add_argument('-o2', dest="Path2Output", type=str, default="./Output", help="PATH for other temporary and output files") 
# optional switches
parser.add_argument('-u', '--updatedBHPs', action="store_true", default=False, help="use updated <BestHitPairs> files; see below")
parser.add_argument('-L', '--Long', action="store_true", default=False, help="see below")
parser.add_argument('-V', '--Verbose', action="store_true", default=False, help="see below")
# options to set parameters to determine co-linearity (see the synopsis):
parser.add_argument('-W', dest="window_size", type=int, default=20, help="default=20; see 'CL_finder.py'") 
parser.add_argument('-N', dest="num_CL_trshld", type=int, default=4, help="default=4; see 'CL_finder.py'")
parser.add_argument('-G', dest="gap_CL_trshld", type=int, default=20, help="default=20; see 'CL_finder.py'")

args = parser.parse_args()

# parameters for 'CL_finder.py'
window_size = args.window_size
num_CL_trshld = args.num_CL_trshld
gap_CL_trshld = args.gap_CL_trshld

# defining PATHs and create Output directory, if not already exisiting
path_TDfiles = args.Path2TDfiles
if path_TDfiles[-1] != "/": path_TDfiles = path_TDfiles + "/"
path_BestHitPairs = args.Path2BHPs
if path_BestHitPairs[-1] != "/": path_BestHitPairs = path_BestHitPairs + "/"
path_BestHitPairs_fixed = args.Path2BHPs_fxd
if path_BestHitPairs_fixed[-1] != "/": path_BestHitPairs_fixed = path_BestHitPairs_fixed + "/"
path_input = args.Path2Input
if path_input[-1] != "/": path_input = path_input + "/"
path_output = args.Path2Output
if path_output[-1] != "/": path_output = path_output + "/"


try: 
	os.makedirs(path_output)
except OSError:
	if not os.path.isdir(path_output): raise

try: 
	os.makedirs(path_BestHitPairs_fixed)
except OSError:
	if not os.path.isdir(path_BestHitPairs_fixed): raise

# defining expected format of input and output file names.  modify as needed: 
input_BestHitPairs_filename_format = path_BestHitPairs + "BestHits__%s_vs_%s.list" 
output_BestHitPairs_filename_format = path_BestHitPairs_fixed + "BestHits__%s_vs_%s.list" 

print "###### Beginning 'update_OrthNet_after_mcl.py' "


########################################
### 1. reading the species list file ###
########################################
try:
	fin_SpcsList = open(args.Project + '.list', 'r')
except OSError:
	fin_SpcsList = open(path_input + args.Project + '.list')
spcsID_list = []

print "\nreading the list file:" + fin_SpcsList.name
for line in fin_SpcsList:
	spcsID_list.append(line.strip())
	print line.strip()
fin_SpcsList.close()
print "Total %d species IDs detected." % (len(spcsID_list))

## v0.3.8: this is to accept both old and new BHP file name formats ... why I am bothering with this ...
if not os.path.isfile(input_BestHitPairs_filename_format % (spcsID_list[0], spcsID_list[1])):
	if os.path.isfile(path_BestHitPairs + "BestHits__%s__vs__%s.list" % (spcsID_list[0], spcsID_list[1])):
		input_BestHitPairs_filename_format = path_BestHitPairs + "BestHits__%s__vs__%s.list" 


################################################################
### 2. reading <mclOutput_parsed> and <Input>.clusterd.edges ###
################################################################

## 2.1. reading <mclOutput_parsed> 
nodeID_clusterID_affected_by_MCL_dict = dict() # dict with key = nodeID (spcsID|geneID), value=clusterID

print "Begin reading %s." % args.mclOutput_parsed.name

for line in args.mclOutput_parsed:
	tok = line.split('\t')
	if tok[0].strip() not in nodeID_clusterID_affected_by_MCL_dict:
		nodeID_clusterID_affected_by_MCL_dict[tok[0].strip()] = tok[1].strip()

print "Finished reading %s." % args.mclOutput_parsed.name
args.mclOutput_parsed.close()


## 2.2. reading <Input>.clusterd.edges, collecting queryIDs that require new subject
fin_edges = open(path_input + args.Project + ".clstrd.edges", "rU")
fout_TD_edges_report = open(path_output + args.Project + ".TD_edges_affected_by_mcl.report.txt", "w`")
fout_rc_edges_report = open(path_output + args.Project + ".RC_edges_affected_by_mcl.report.txt", "w`")

node1ID = "" # assumes the format of 'spcsID|geneID'
node2ID = "" # assumes the format of 'spcsID|geneID'
CLtype_12 = "" # CL_type[node1ID -> node2ID] 
CLtype_21 = "" # CL_type[node2ID -> node1ID]
num_edges = 0 
num_TD_edges = 0

num_TD_edges_need_fixing = 0
num_rc_edges_need_fixing = 0
num_nrc_edges_need_fixing = 0

nodeID_requring_NewBestHit_dict = dict() # dict with key = queryID (spcsID|geneID), value = subject_spcs

for line in fin_edges:
	tok = line.split('\t')
	node1ID = tok[1].strip()
	node2ID = tok[2].strip()
	CLtype_12 = tok[3].strip()
	CLtype_21 = tok[4].strip()

	# counting number of edges in <Input>.clstrd.edges; count TD edges separately 
	if CLtype_12 == 'TD' : num_TD_edges += 1
	elif CLtype_12 == '-' or CLtype_21 != '-' : num_edges += 1
	else : num_edges += 2
	
	if node1ID in nodeID_clusterID_affected_by_MCL_dict and node2ID in nodeID_clusterID_affected_by_MCL_dict:
		if nodeID_clusterID_affected_by_MCL_dict[node1ID] != nodeID_clusterID_affected_by_MCL_dict[node2ID]:
			if CLtype_12 == 'TD':
				num_TD_edges_need_fixing += 1
				fout_TD_edges_report.write( "TD_edge: %s in %s; %s in %s\n" % ( node1ID, nodeID_clusterID_affected_by_MCL_dict[node1ID], node2ID, nodeID_clusterID_affected_by_MCL_dict[node2ID]))
			elif CLtype_12 == '-':
				nodeID_requring_NewBestHit_dict[node2ID] = node1ID.split('|')[0].strip()
				num_nrc_edges_need_fixing += 1
			elif CLtype_21 == '-':
				nodeID_requring_NewBestHit_dict[node1ID] = node2ID.split('|')[0].strip()
				num_nrc_edges_need_fixing += 1
			else:
				nodeID_requring_NewBestHit_dict[node1ID] = node2ID.split('|')[0].strip()
				nodeID_requring_NewBestHit_dict[node2ID] = node1ID.split('|')[0].strip()
				num_rc_edges_need_fixing += 1
				fout_rc_edges_report.write( "rc_edge: %s in %s; %s in %s ; fwd %s, rev %s\n" % ( node1ID, nodeID_clusterID_affected_by_MCL_dict[node1ID], node2ID, nodeID_clusterID_affected_by_MCL_dict[node2ID], CLtype_12, CLtype_21) )
			
print "Finished reading %s and identified %d TD edges and %d edges between species, before mcl; " % \
			(fin_edges.name, num_TD_edges, num_edges )
print "Total %d TD edges, %d non-rc edges, and %d rc edges were removed by mcl; %d nodes need updating;" % \
			(num_TD_edges_need_fixing, num_nrc_edges_need_fixing, num_rc_edges_need_fixing, len(nodeID_requring_NewBestHit_dict) )


###################################################
### 3. starting the update of BestHitPairs files###
###################################################
queryID = "" # assumes the format of 'spcsID|geneID'
subjectID = "" # assumes the format of 'spcsID|geneID'

query_spcsID = ""
subject_spcsID = ""

queryID_gene = ""
queryID_gene_prev = ""
subjectID_gene = ""
subjectID_gene_prev = ""
subjectID_gene_updated = ""
BestHit_rank = 0

BestHitPair_processed = False
BestHitPair_written = False
FirstEntry = True

num_BestHitPairs_fxd = 0

for i in range( len(spcsID_list) ):
	query_spcsID = spcsID_list[i]
	subject_spcsID = ""

	for j in range( len(spcsID_list) ):
		if spcsID_list[j] != query_spcsID:
			subject_spcsID = spcsID_list[j]
			
			## opening the BestHitPairs files for reading and writing
			fin_BestHitPairs = open(input_BestHitPairs_filename_format % (query_spcsID, subject_spcsID), "rU")
			fout_BestHitPairs = open(output_BestHitPairs_filename_format % (query_spcsID, subject_spcsID), "w")
			print "updating %s and writing to %s," % (fin_BestHitPairs.name, fout_BestHitPairs.name)
			
			for line in fin_BestHitPairs:
				tok = line.split('\t')
				queryID_gene = tok[0].split('|')[ len(tok[0].split('|')) - 1 ].strip()
				subjectID_gene = tok[1].split('|')[ len(tok[1].split('|')) - 1 ].strip()
				if queryID_gene != queryID_gene_prev:
					# process if there is no better 'BestHit' for the previous queryID_gene
					# in this case, the first BestHit will be repeated at the end, with the 'BHx' tag 
					if BestHitPair_written == False and FirstEntry == False:
						fout_BestHitPairs.write(queryID_gene_prev + '\t' + subjectID_gene_marked + '\t' + 'BHmx' + '\n')
					if FirstEntry == True:
						FirstEntry = False
					
					queryID = query_spcsID + '|' + queryID_gene
					subjectID_gene_marked = subjectID_gene
					BestHit_rank = 1
					BestHitPair_processed = False
					BestHitPair_written = False
					# check whether no update is needed at this stage:
					if queryID not in nodeID_requring_NewBestHit_dict:
						BestHitPair_processed = True
					elif nodeID_requring_NewBestHit_dict[queryID] != subject_spcsID:
						BestHitPair_processed = True
				elif subjectID_gene != subjectID_gene_prev and BestHitPair_processed == False:
					BestHit_rank = BestHit_rank + 1
					subjectID = subject_spcsID + '|' + subjectID_gene
					if subjectID in nodeID_clusterID_affected_by_MCL_dict:
						if nodeID_clusterID_affected_by_MCL_dict[queryID] == nodeID_clusterID_affected_by_MCL_dict[subjectID]:
							subjectID_gene_marked = subjectID_gene
							BestHitPair_processed = True

				# mark the updated BestHitPair, removing duplicated entries
				if BestHitPair_processed == True and BestHitPair_written == False:
					fout_BestHitPairs.write(queryID_gene + '\t' + subjectID_gene_marked + '\tBHm' + str(BestHit_rank) + '\n')
					if BestHit_rank > 1 :
						num_BestHitPairs_fxd += 1
					BestHitPair_written = True
				elif subjectID_gene != subjectID_gene_prev or queryID_gene != queryID_gene_prev:
					if args.updatedBHPs == True :
						fout_BestHitPairs.write(line)
					else:
						fout_BestHitPairs.write(queryID_gene + '\t' + subjectID_gene + '\n')
					
				queryID_gene_prev = queryID_gene
				subjectID_gene_prev = subjectID_gene
				
			fin_BestHitPairs.close()
			fout_BestHitPairs.close()
			
print "%d BestHitPairs were fixed; updated 'BestHitPairs' files were placed in %s\n" % (num_BestHitPairs_fxd, path_output)
print "starting 'CL_finder_multi.py' again, using updated 'BestHitPairs'\n"


##########################################################################
### 4. re-running 'CL_finder_multi.py' with updated BestHitPairs files ###
##########################################################################
command = "CL_finder_multi.py " + args.Project + " -t " + path_TDfiles + " -T " + args.TDfile_nameFmt \
				+ " -b " + path_BestHitPairs_fixed + " -o " + path_output + " -unr -W " + str(window_size) \
				+ " -N " + str(num_CL_trshld) + " -G " + str(gap_CL_trshld)
print "now running: %s" % command
subprocess.call(command, shell=True)


##################################################################################
### 5. update <Project>.clstrd.edges, based on new 'CL_finder_multi.py -n' run ###
##################################################################################

## 5.1. reading path_output + '<Project>.4OrthNet.input' as fin_4OrthNet_fxd
fin_4OrthNet_fxd = open(path_output + args.Project + ".4OrthNet.input", "rU")
print "reading %s" % fin_4OrthNet_fxd.name

edgeID_CLtype_fxd_dict = dict() # key = "queryID___subjectID" (i.e. "edgeID"), value = CLtype
queryID_subject_spcsID_fxd_dict = dict() # key = "queryID___subject_spcsID", value = subjectID_gene
queryID = ""
subjectID = ""
edgeID = ""

for line in fin_4OrthNet_fxd:
	tok = line.split()
	queryID = tok[0].strip()
	subjectID = tok[1].strip()
	edgeID = queryID + '___' + subjectID
	edgeID_CLtype_fxd_dict[ edgeID ] = tok[2].strip()
	queryID_subject_spcsID_fxd_dict[ queryID + '___' + subjectID.split('|')[0] ] = subjectID.split('|')[1]
	
## 5.2. updating './<Project>.clstrd.edges' to './<Project>.clstrd.afterMCL.edges' 
fin_edges.seek(0)
fout_edges_fxd = open(path_output + args.Project + ".clstrd.afterMCL.edges.temp", "w")
print "updating %s and writing to %s" % (fin_edges.name, fout_edges_fxd.name)

clusterID = ""
node1ID = "" # assumes the format of 'spcsID|geneID'
node2ID = "" # assumes the format of 'spcsID|geneID'
CLtype_12 = "" # CL_type[node1ID -> node2ID] 
CLtype_21 = "" # CL_type[node2ID -> node1ID]
header = True

edgeID_12_fxd = ""
edgeID_21_fxd = ""

for line in fin_edges:
	if header == True:
		header = False
	else:
		tok = line.split('\t')
		clusterID = tok[0].strip()
		node1ID = tok[1].strip()
		node2ID = tok[2].strip()
		CLtype_12 = tok[3].strip()
		CLtype_21 = tok[4].strip()
		
		# decision tree: if the cluster was not modified by mcl:
		if node1ID not in nodeID_clusterID_affected_by_MCL_dict and node2ID not in nodeID_clusterID_affected_by_MCL_dict:
			fout_edges_fxd.write( clusterID + '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + '\n' )
			#print( clusterID + '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + ' ##case0' )
		# decision tree: if the cluster was modified by mcl, but the edge was not:
		elif nodeID_clusterID_affected_by_MCL_dict.get(node1ID, "NA1") == nodeID_clusterID_affected_by_MCL_dict.get(node2ID, "NA2"):
			fout_edges_fxd.write( nodeID_clusterID_affected_by_MCL_dict[node1ID] \
									+ '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + '\n' )
			#print( nodeID_clusterID_affected_by_MCL_dict[node1ID] \
			#						+ '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + ' ##case1' )
		# decision tree: if both the cluster and the edge modified by mcl, and the edge happen to be a TD edge, ...:
		elif node1ID.split('|')[0] == node2ID.split('|')[0]:
			fout_TD_edges_report.write( "TD edge affected by mcl: nd1=%s, nd2=%s, clstr(nd1)=%s, clstr(nd2)=%s\n" % (node1ID, node2ID, nodeID_clusterID_affected_by_MCL_dict.get(node1ID,'NA?'), nodeID_clusterID_affected_by_MCL_dict.get(node2ID,'NA?') ) )# report to a separate file, but do not include to .edge files yet?? or include to both? 
			if args.Verbose:
				print( "TD edge affected by mcl: nd1=%s, nd2=%s, clstr(nd1)=%s, clstr(nd2)=%s" % (node1ID, node2ID, nodeID_clusterID_affected_by_MCL_dict.get(node1ID,'NA?'), nodeID_clusterID_affected_by_MCL_dict.get(node2ID,'NA?') ) ) # report to a separate file, but do not include to .edge files yet?? or include to both? 
		# decision tree: if both the cluster and the edge modified by mcl, see whether the edge can be rescued by the updated BHpairs:	
		else:
			edgeID_12_fxd = "-___-"
			edgeID_21_fxd = "-___-"
			if node1ID + '___' + node2ID.split('|')[0] in queryID_subject_spcsID_fxd_dict:
				edgeID_12_fxd = node1ID + '___' + node2ID.split('|')[0] + '|' + \
							queryID_subject_spcsID_fxd_dict[ node1ID + '___' + node2ID.split('|')[0] ]
				if edgeID_12_fxd != (node1ID + '___' + node2ID) and args.Verbose:
					print "### edgeID_12 %s___%s is modified to edgeID_12_fxd %s" % (node1ID, node2ID, edgeID_12_fxd)
			if node2ID + '___' + node1ID.split('|')[0] in queryID_subject_spcsID_fxd_dict:
				edgeID_21_fxd = node2ID + '___' + node1ID.split('|')[0] + '|' + \
							queryID_subject_spcsID_fxd_dict[ node2ID + '___' + node1ID.split('|')[0] ]
				if edgeID_21_fxd != (node2ID + '___' + node1ID) and args.Verbose:
					print "### edgeID_21 %s___%s is modified to edgeID_21_fxd %s" % (node2ID, node1ID, edgeID_21_fxd)
			# process the fwd edge
			if edgeID_12_fxd != "-___-":
				node1ID = edgeID_12_fxd.split('___')[0]
				node2ID = edgeID_12_fxd.split('___')[1]
				if nodeID_clusterID_affected_by_MCL_dict.get(node1ID, "NA1") == nodeID_clusterID_affected_by_MCL_dict.get(node2ID, "NA2"):
					CLtype_12 = edgeID_CLtype_fxd_dict[ edgeID_12_fxd ]
					CLtype_21 = '-'
					fout_edges_fxd.write( nodeID_clusterID_affected_by_MCL_dict[node1ID] + '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + '\n' )
					if args.Verbose:
						print( nodeID_clusterID_affected_by_MCL_dict[node1ID] + '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + ' ##case3fwd' )
				elif args.Verbose:
					print "dropping edgeID_12_fxd fwd %s, nd1 and nd2 belong to different clusters, %s and %s" % (edgeID_12_fxd, nodeID_clusterID_affected_by_MCL_dict.get(node1ID, "NA1"), nodeID_clusterID_affected_by_MCL_dict.get(node2ID, "NA2"))
			# process the rev edge
			if edgeID_21_fxd != "-___-":
				node1ID = edgeID_21_fxd.split('___')[1]
				node2ID = edgeID_21_fxd.split('___')[0]
				if nodeID_clusterID_affected_by_MCL_dict.get(node1ID, "NA1") == nodeID_clusterID_affected_by_MCL_dict.get(node2ID, "NA2"):
					CLtype_12 = '-'
					CLtype_21 = edgeID_CLtype_fxd_dict[ edgeID_21_fxd ]
					fout_edges_fxd.write( nodeID_clusterID_affected_by_MCL_dict[node1ID] + '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + '\n' )
					if args.Verbose:			
						print( nodeID_clusterID_affected_by_MCL_dict[node1ID] + '\t' + node1ID + '\t' + node2ID + '\t' + CLtype_12 + '\t' + CLtype_21 + ' ##case3rev' )	
				elif args.Verbose:
					print "dropping edgeID_12_fxd rev %s, nd1 and nd2 belong to different clusters, %s and %s" % (edgeID_12_fxd, nodeID_clusterID_affected_by_MCL_dict.get(node1ID, "NA1"), nodeID_clusterID_affected_by_MCL_dict.get(node2ID, "NA2"))

print "\nfinished writing %s, now consolidating redundant edges," % fout_edges_fxd.name						
fin_edges.close()
fout_edges_fxd.close()

## 5.3. consolidating redundant edges to genearate './<Project>.clstrd.afterMCL.edges'
fin_edges_fxd = open(path_output + args.Project + ".clstrd.afterMCL.edges.temp", "rU")

edgeID_clusterID_dict = dict() # key = "node1ID___node2ID" (i.e. "edgeID"), value = clusterID
edgeID_CLtype12_dict = dict() # key = "node1ID___node2ID" (i.e. "edgeID"), value = CLtype_12
edgeID_CLtype21_dict = dict() # key = "node1ID___node2ID" (i.e. "edgeID"), value = CLtype_21

clusterID = ""
edgeID = ""
CLtype_12 = ""
CLtype_21 = ""
num_problematic_edges = 0

for line in fin_edges_fxd:
	tok = line.split('\t')
	clusterID = tok[0].strip()
	edgeID = tok[1].strip() + '___' + tok[2].strip()
	CLtype_12 = tok[3].strip()
	CLtype_21 = tok[4].strip()
	
	if edgeID not in edgeID_clusterID_dict:
		edgeID_clusterID_dict[edgeID] = clusterID
	elif edgeID_clusterID_dict[edgeID] != clusterID:
		if args.Verbose:
			print "warning: edgeID %s appears in more than one cluster: %s and %s" % (edgeID, edgeID_clusterID_dict[edgeID], clusterID)
		num_problematic_edges += 1
		
	if CLtype_12 != '-':
		if edgeID not in edgeID_CLtype12_dict:
			edgeID_CLtype12_dict[edgeID] = CLtype_12
		elif re.sub('_BH.*1$', '', edgeID_CLtype12_dict[edgeID].strip()) != re.sub('_BH.*1$', '', CLtype_12):
			if args.Verbose:
				print "warning: edgeID %s contains conflicting CLtype[1->2]: %s and %s" % (edgeID, edgeID_CLtype12_dict[edgeID], CLtype_12)
			num_problematic_edges += 1
		
	if CLtype_21 != '-':
		if edgeID not in edgeID_CLtype21_dict:
			edgeID_CLtype21_dict[edgeID] = CLtype_21
		elif re.sub('_BH.*1$', '', edgeID_CLtype21_dict[edgeID].strip()) != re.sub('_BH.*1$', '', CLtype_21):
			if args.Verbose:
				print "warning: edgeID %s contains conflicting CLtype[2->1]: %s and %s" % (edgeID, edgeID_CLtype21_dict[edgeID], CLtype_21)
			num_problematic_edges += 1

print "found %d problematic edges (warnings)... run with -V option to see what they are" % num_problematic_edges 
fin_edges_fxd.close()
fout_edges_fxd_consolidated = open(path_output + args.Project + ".clstrd.afterMCL.edges", "w")

fout_edges_fxd_consolidated.write("clusterID\tnode1ID\tsubjectID\tCLtype[1->2]\tCLtype[2->1]\n")

for key in edgeID_clusterID_dict:
	node1ID = key.split('___')[0]
	node2ID = key.split('___')[1]
	fout_edges_fxd_consolidated.write(edgeID_clusterID_dict[key] + '\t' + node1ID + '\t' + node2ID + '\t' + edgeID_CLtype12_dict.get(key, '-') + '\t' + edgeID_CLtype21_dict.get(key, '-') + '\n' )

fout_edges_fxd_consolidated.close()

# sorting to create the final '.afterMCL.edges' file
command = "awk 'NR == 1; NR > 1 {print $0 | \"sort -k1,1 -k2,2 -k3,3\"}' " + path_output + args.Project + ".clstrd.afterMCL.edges > __edges_temp__; mv __edges_temp__ " \
		+ path_output + args.Project + ".clstrd.afterMCL.edges"
subprocess.call(command, shell=True)


###########################################################################################
### 6. update <Project>.clstrd.nodes, based on the <Project>.clstrd.afterMCL.edges file ###
###########################################################################################
## 6.1. create './<Project>.clstrd.afterMCL.nodes', './<Project>.clstrd.afterMCL.nodeCounts', and './<Project>.clstrd.afterMCL.nodes.mclOutput' 
print "\nfinished updating .edges files; now writing .nodes, .nodeCounts, and .nodes.mclOutput files\n"
fin_edges_fxd_consolidated = open(path_output + args.Project + ".clstrd.afterMCL.edges", "rU")
fout_nodes_fxd = open(path_output + args.Project + ".clstrd.afterMCL.nodes", "w") 
fout_nodeCounts_fxd = open(path_output + args.Project + ".clstrd.afterMCL.nodeCounts", "w")
fout_nodes_fxd_mclOutput = open(path_output + args.Project + ".clstrd.afterMCL.nodes.mclOutput", "w") 

nodeID_clusterID_afterMCL_dict = dict() # key = 'nodeID', value = clusterID as in '<Project>.clstrd.afterMCL.edges'
num_nodeID_as_query_dict = dict() # key = 'nodeID' (i.e. 'spcsID|geneID'), value = counts as query
num_nodeID_as_subject_dict = dict() # key = 'nodeID' (i.e. 'spcsID|geneID'), value = counts as query

clusterID = ""
node1ID = ""
node2ID = ""
CLtype_12 = ""
CLtype_21 = ""
header = True

# wrting headers
fout_nodes_fxd.write("nodeID\tclusterID\tcount_as_query\tcount_as_subject\tcount_as_both\n")
fout_nodeCounts_fxd.write("clusterID\t#Nodes\tNodeCounts_" + '.'.join(spcsID_list) + '\n' )

## 6.2. counting node occurrence from the final '.edges' file 
for line in fin_edges_fxd_consolidated:
	tok = line.split('\t')

	if header:
		header = False
	else:
		clusterID = tok[0].strip()
		node1ID = tok[1].strip()
		node2ID = tok[2].strip()
		CLtype_12 = tok[3].strip()
		CLtype_21 = tok[4].strip()

		if node1ID not in nodeID_clusterID_afterMCL_dict:
			nodeID_clusterID_afterMCL_dict[node1ID] = clusterID
		elif clusterID != nodeID_clusterID_afterMCL_dict[node1ID] and args.Verbose:
			print "warning: nd1 %s appears in different clusters, %s and %s" % (node1ID, nodeID_clusterID_afterMCL_dict[node1ID], clusterID)
		if node2ID not in nodeID_clusterID_afterMCL_dict:
			nodeID_clusterID_afterMCL_dict[node2ID] = clusterID
		elif clusterID != nodeID_clusterID_afterMCL_dict[node2ID] and args.Verbose:
			print "warning: nd2 %s appears in different clusters, %s and %s" % (node2ID, nodeID_clusterID_afterMCL_dict[node2ID], clusterID)
		
		if CLtype_12 != 'TD' and CLtype_12 != '-':
			num_nodeID_as_query_dict[node1ID] = num_nodeID_as_query_dict.get(node1ID, 0) + 1
			num_nodeID_as_subject_dict[node2ID] = num_nodeID_as_subject_dict.get(node2ID, 0) + 1
		if CLtype_21 != 'TD' and CLtype_21 != '-':
			num_nodeID_as_query_dict[node2ID] = num_nodeID_as_query_dict.get(node2ID, 0) + 1
			num_nodeID_as_subject_dict[node1ID] = num_nodeID_as_subject_dict.get(node1ID, 0) + 1
			
for key in nodeID_clusterID_afterMCL_dict:
	if key not in num_nodeID_as_query_dict: num_nodeID_as_query_dict[key] = 0
	if key not in num_nodeID_as_subject_dict: num_nodeID_as_subject_dict[key] = 0

## 6.2. print the '.afterMCl.nodes' file, and count spcsIDs in nodeIDs for each clusterID
spcsID = ""
nodeID = ""

num_spcsID_in_clusterID_dict = dict() # key = clusterID, value = dict of node counts for each spcsID 
for nodeID in nodeID_clusterID_afterMCL_dict:
	clusterID = nodeID_clusterID_afterMCL_dict[nodeID]
	# print '.afterMCL.nodes' file
	fout_nodes_fxd.write(nodeID + '\t' + clusterID + '\t' + str(num_nodeID_as_query_dict[nodeID]) + '\t' + str(num_nodeID_as_subject_dict[nodeID]) \
							+ '\t' + str(num_nodeID_as_query_dict[nodeID] + num_nodeID_as_subject_dict[nodeID]) + '\n' )
	# count spcsIDs for '.afterMCL.nodeCounts' file 
	if clusterID not in num_spcsID_in_clusterID_dict:
		num_spcsID_in_clusterID_dict[clusterID] = dict() # key = spcsID, value = count
	spcsID = nodeID.split('|')[0].strip()
	num_spcsID_in_clusterID_dict[clusterID][spcsID] = num_spcsID_in_clusterID_dict[clusterID].get(spcsID, 0) + 1

## 6.3. print the '.afterMCl.nodeCounts' file
num_spcsID_in_clusterID_list = []
num_total_nodes = 0
for clusterID in sorted(num_spcsID_in_clusterID_dict):
	num_spcsID_in_clusterID_list = []
	num_total_nodes = 0
	for spcsID in spcsID_list:
		num_spcsID_in_clusterID_list.append(str(num_spcsID_in_clusterID_dict[clusterID].get(spcsID, 0)))
		num_total_nodes += num_spcsID_in_clusterID_dict[clusterID].get(spcsID, 0)
	fout_nodeCounts_fxd.write(clusterID +'\t%d\t%s\n' % ( num_total_nodes, '.'.join(num_spcsID_in_clusterID_list) ) )

fin_edges_fxd_consolidated.close()
fout_nodes_fxd.close()
fout_nodeCounts_fxd.close()

## 6.4 sorting '.afterMCL.nodes' file, to create the final version
command = "awk 'NR == 1; NR > 1 {print $0 | \"sort -k2,2 -k5,5nr\"}' " + path_output + args.Project + ".clstrd.afterMCL.nodes > __nodes_temp__; mv __nodes_temp__ " \
		+ path_output + args.Project + ".clstrd.afterMCL.nodes"
subprocess.call(command, shell=True)

## 6.5. print the '.afterMCL.nodes.mclOutput' file
clusterID_nodeID_afterMCL_dict = dict() # key = clusterID, value = list of nodeIDs

for ndID in nodeID_clusterID_afterMCL_dict:
	clID = nodeID_clusterID_afterMCL_dict[ndID]
	if clID not in clusterID_nodeID_afterMCL_dict:
		clusterID_nodeID_afterMCL_dict[clID] = [ndID]
	else:
		clusterID_nodeID_afterMCL_dict[clID].append(ndID)
		
for clID in sorted(clusterID_nodeID_afterMCL_dict):
	fout_nodes_fxd_mclOutput.write( clID + ": " + ' '.join(sorted(clusterID_nodeID_afterMCL_dict[clID])) + '\n')
	
fout_nodes_fxd_mclOutput.close()


#######################################################
### 7. add OrthNet information to CLfm output files ###
#######################################################
## 7.1 read CLfm_output_filename_list
fin_CLfm_output_list = open(path_output + args.Project + "_CLfm_output.list", 'r')
CLfm_output_filename_list = []
for line in fin_CLfm_output_list:
	CLfm_output_filename_list.append(line.strip())
fin_CLfm_output_list.close()

## 7.2 add OrthNet information as additional columns to CLfm output files
spcs_viewpoint = ""
nodeID = ""
clusterID = ""
CLmatrix_clean = ""
line_afterMCL = ""
num_spcsID_in_clusterID_list = []
num_nodes = 0
header = True

for item in CLfm_output_filename_list:
	# open files for reading and writing
	fin_CLfm_output = open(path_output + item.split('/')[-1], 'r')
	fout_CLfm_output_afterMCL = open(path_output + item.split('/')[-1].strip('.txt') + '.afterMCL.txt', 'w')
	print "adding OrthNet info to %s and writing to %s." % (fin_CLfm_output.name, fout_CLfm_output_afterMCL.name) 	
	header = True
	# get spcsID
	spcs_viewpoint = item.split('/')[-1].split('.CL_compared2')[0]	
	
	for line in fin_CLfm_output:
		if args.Long == False:
			line_afterMCL = '\t'.join(line.split('\t')[0:-1]) # remove the CLmatrix* column to make the output simple	
		if header == True:
			if args.Long:
				fout_CLfm_output_afterMCL.write(line.strip() + '\tOrthNetID\t#asQuery\t#asSubject\tnodeCounts\t#nodes\tOrthNet_pattern\n')
			else:
				fout_CLfm_output_afterMCL.write(line_afterMCL + '\tOrthNetID\t#nodes\tOrthNet_pattern\n')			
			header = False
		else:
			# get the CLmatrix cleaned of '_BHmX', '_BHX', '_end', etc.
			CLmatrix_clean = re.sub("(_BHm[1-9][0-9])|(_BHm[1-9x])|(_BH[1-9][0-9])|(_BH[1-9x])|(_end)|(_bestHitNotPC)", "", line.split('\t')[-1].strip())
			nodeID = spcs_viewpoint + '|' + line.split('\t')[0].strip()
			clusterID = nodeID_clusterID_afterMCL_dict.get(nodeID, "")
			if args.Long:
				line_afterMCL = line.strip() + '\t' + clusterID + '\t' + str(num_nodeID_as_query_dict.get(nodeID, "")) \
							 + '\t' + str(num_nodeID_as_subject_dict.get(nodeID, ""))
			else:
				line_afterMCL = line_afterMCL + '\t' + clusterID			
			if clusterID != "":
				num_spcsID_in_clusterID_list = []
				num_nodes = 0
				for spcsID in spcsID_list:
					num_spcsID_in_clusterID_list.append(str(num_spcsID_in_clusterID_dict[clusterID].get(spcsID, 0)))
					num_nodes = num_nodes + num_spcsID_in_clusterID_dict[clusterID].get(spcsID, 0)
				if args.Long:
					line_afterMCL = line_afterMCL + '\t' + '.'.join(num_spcsID_in_clusterID_list)
					line_afterMCL = line_afterMCL + '\t' + str(num_nodes)
					line_afterMCL = line_afterMCL + '\t' + CLmatrix_clean + '__' + '.'.join(num_spcsID_in_clusterID_list) + '\n'
				else:
					line_afterMCL = line_afterMCL + '\t' + str(num_nodes)
					line_afterMCL = line_afterMCL + '\t' + CLmatrix_clean + '__' + '.'.join(num_spcsID_in_clusterID_list) + '\n'				
			elif args.Long:
				line_afterMCL = line_afterMCL + '\t\t\t' + CLmatrix_clean + '\n'
			else:
				line_afterMCL = line_afterMCL + '\t\t' + CLmatrix_clean + '\n'
			fout_CLfm_output_afterMCL.write(line_afterMCL)
	fin_CLfm_output.close()
	fout_CLfm_output_afterMCL.close()

print "\ndone"

## clean the .temp file
try:
    os.remove(path_output + args.Project + ".clstrd.afterMCL.edges.temp")
except OSError:
    pass
